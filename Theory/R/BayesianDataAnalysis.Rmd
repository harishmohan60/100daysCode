---
title: "Bayesian Data Analysis"
author: "Harish Mohanaselvam"
date: "10/24/2020"
output: rmarkdown::github_document
---

### Bayesian Inference in a nutshell
A method for figuring out un-observable quantities given known facts that uses probability to describe the uncertainty over what the values of the unknown quantities could be. 

Bayesian Data Analysis
1. The use of Bayesian inference to lean from data
2. Can be used for hypothesis testing, linear regression, etc. 
3. Is flexible and allow you to construct problem-specific models

Chapter1: A small Bayesian analysis
Chapter2: How Bayesian inference works
chapter3: Why you would want to use Bayesian data analysis?
chapter4: Wrapping up + practical tool for Bayesian Data Analysis in R

## Unknowns and ice cream
Bayesian inference is a method for figuring out unknown or un-observable given known facts. In the case of the Enigma machine, Alan Turing wanted to figure out
the unknown settings of the wheels ans ultimately the meaning of the coded messages. 

When analysis data, we are also interested in learning about unknown quantities. For example, say that we are interested in how daily ice cream sales relate to the temperature, and we decide the use of linear regression to investigate this. 

Which of the following quantities could be considered unknown in this case? 
1. The slope of the underlying regression line
2. How many ice cream sales vary on days with a similar temperature?
3. How many ice cream we will sell tomorrow given a forecast of 27C?

### Background
Probability
- A number between 0 and 1
- A statement about certainty/ uncertainty
- 1 is complete certainly something is the case
- O is complete certainly something is not the case
- Not only about the yes/no events

A Bayesian model for the proportion of success
Flows -->> 
prop_model(data)
- The data is a vector of successes and failures represented by 1s and 0s
- There is an unknown underlying proportion of success
- If data point is a success is only affected by this proportion
- Prior to seeing any data, any underlying proportion of success is equally likely
- The result is a probability distribution that represents what the model knows about the underlying proportion of success

### Coin flips with prop_model
The function prop_model has been loaded into your workplace. It implements a Bayesian model that assumes that:
- The data is a vector of successes and failure represented by 1s and 0s
- There is an unknown underlying proportion of success
- Prior to being updated with data any underlying proportion of success is equally likely.

Assume you just flipped a coin four times and the result was heads, tails, tails, heads. If you code heads as a success and tails
as a failure then the following R code runs prop_model with the data.

Looking at the final probability distribution at n=4, what information does the model have regarding the underlying proportion of head? 
- it's mostly likely around 50%, but there is large uncertainly

### Zombie drugs with prop_model
If we really were interested in the underlying proportion of heads of the coin then prop_model isn't partially useful. 
Since it assumes that any underlying proportion of success is equally likely prior to seeing any data it will take a lot of coin flipping to convince
prop_model that the coin is fair. This model is more appropriate in a situation where we have little background knowledge about the underlying 
proportion of success. 

Let's say the zombie apocalypse is upon us and we have come up with a new experiment drug to cure zombies. We have no clue how effective it's going to be, but when we gave it to 13 zombies two of them turned humans again. 

- Change the data argument to prop_model to estimate the underlying proportion of success of curing a zombie. 

The model implemented is pop_model makes more sense here as we had no clue how good the drug would be. The final probability distribution (at n=13)
represents what the model now knows about the underlying proportion of cured zombies. What proportion of zombies would we expect to turn human if we
administered this new drug to the whole zombie population
- between 5% to 40%

# Prior and Posterior
- A prior is a probability distribution that represents what the model knows before seeing the data
- A posterior is a probability distribution that represents what the model knows after having seen the data

### Looking at samples from prop_model
Here again is the prop_model function which has been given the data from our zombie experiment where two out of 13 zombies got cured. In additional
to producing a plot, prop_model also returns a large random sample from the posterior over the underlying proportion of success. 

Assign the return value of prop_model to a variable called posterior and take a look at the first number of samples using the command head(posterior)

Looking at these first few samples confirms what is already show in the plot. That the underlying proportion of cured Zombies is likely somewhere between 
5% and 50%. But these were just the first six samples in posterior which currently contain 10,000 samples (the default of prop_model). 
- Take a look at the distribution of all the samples in posterior by plotting it as a histogram using the hist() function with posterior as the fist argument

Compare this histogram to the plot produced directly by prop_model (you'll find it if you go to the previous plot in the Plots tab). You should notice that 
histogram and the posterior distribution (at n = 13) describes the same distribution. 

OK, so you got a vector of samples from prop_model and confirmed that they are so good representation of the posterior distribution. What have you gained by doing this? So far, not much, but the next exercise will show you some examples of why samples are so convenient to work with. But before tou leave:

### Summarizing the zombie drug experiments 
The point of working this samples from a probability distribution is that it makes it easy to calculate new measures if interest. The following tasks are about doing just this! 

A point estimate is a single number used to summarize what's known about a parameter of interest. It can be seen as a "best guess" of the value of the parameter. A commonly used point estimate is the median of the posterior. It's the midpoint of the distribution, and it's equally probably for the distribution, and it's equally probably for the parameter value to be larger than the median as it is to be smaller that it. 

So, a best guess is that the drug would care around 18% of all zombies. Another common summary is to report an interval that includes the parameter
of interest with a certain probability. This is called a credible interval (CI). With a posterior represented as a vector of samples you can calculate a CI using the Quantile() function.

quantile() takes the vector of samples as its first argument and the second argument is a vector defining how much probability should be left below and above the CI. For example, the vector c(0.05, 0.95) would yield a 90% CI and c(0.25, 0.75)would yield a 50% CI. 

According to the credible interval, there is a 90% probability that the proportion of zombies the drug would cure is between 6% and 38%. (Here we have to be careful to remember that the percentage of cured zombies and the percentage of probability are two different things.)

Now, there is a rival zombies lab that is also working on a drug. They claim that they are certain that their drug cures 7% of the zombies it's administered to. Can we calculate how probable it is that our drug is better? Yes, we can! But it's a two stage process. 

- First, use the sum to count how many samples in posterior that are larger than 7%. Do this by giving posterior > 0.07 as the argument to sum.
- To turn this count into a probability we now need to normalize it, that is, divide it by the total number of samples in posterior. 
- Divide the result of sum by the number of samples in posterior
- You can get the number of samples in posterior using the length function

The result in a journal
Given the data of two cured and 11 relapsed zombies, and using the Bayesian model described before, there is a 90% probability that our drug cures between 6% and 39% of treated zombies. Further, there  is 93% probability that our drug cures zombies at a higher rate than the current state of the art drug. 


Bayesian Inference
- Data
- Generative model 
- Priors

What is a generative model? 
Its any kind of computer program, mathematical expression, or set of rules that you can feed fixed paramete values and that you can use to generate simulated data- Uniform distribution

```{r}
# Parameters
prop_success <- 0.15
n_zombies <- 13

# Simulating data
data <- c()
for(zombies in 1:n_zombies){
  data[zombies] <- runif(1, min=0, max = 1) < prop_success
}

data <- as.numeric(data)
print(data)
```

```{r}



```



