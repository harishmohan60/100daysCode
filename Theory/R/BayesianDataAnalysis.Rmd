---
title: "Bayesian Data Analysis"
author: "Harish Mohanaselvam"
date: "10/24/2020"
output: html_document: default
---

### Bayesian Inference in a nutshell
A method for figuring out un-observable quantities given known facts that uses probability to describe the uncertainty over what the values of the unknown quantities could be. 

Bayesian Data Analysis
1. The use of Bayesian inference to lean from data
2. Can be used for hypothesis testing, linear regression, etc. 
3. Is flexible and allow you to construct problem-specific models

Chapter1: A small Bayesian analysis
Chapter2: How Bayesian inference works
chapter3: Why you would want to use Bayesian data analysis?
chapter4: Wrapping up + practical tool for Bayesian Data Analysis in R

## Unknowns and ice cream
Bayesian inference is a method for figuring out unknown or un-observable given known facts. In the case of the Enigma machine, Alan Turing wanted to figure out
the unknown settings of the wheels ans ultimately the meaning of the coded messages. 

When analysis data, we are also interested in learning about unknown quantities. For example, say that we are interested in how daily ice cream sales relate to the temperature, and we decide the use of linear regression to investigate this. 

Which of the following quantities could be considered unknown in this case? 
1. The slope of the underlying regression line
2. How many ice cream sales vary on days with a similar temperature?
3. How many ice cream we will sell tomorrow given a forecast of 27C?

### Background
Probability
- A number between 0 and 1
- A statement about certainty/ uncertainty
- 1 is complete certainly something is the case
- O is complete certainly something is not the case
- Not only about the yes/no events

A Bayesian model for the proportion of success
Flows -->> 
prop_model(data)
- The data is a vector of successes and failures represented by 1s and 0s
- There is an unknown underlying proportion of success
- If data point is a success is only affected by this proportion
- Prior to seeing any data, any underlying proportion of success is equally likely
- The result is a probability distribution that represents what the model knows about the underlying proportion of success

### Coin flips with prop_model
The function prop_model has been loaded into your workplace. It implements a Bayesian model that assumes that:
- The data is a vector of successes and failure represented by 1s and 0s
- There is an unknown underlying proportion of success
- Prior to being updated with data any underlying proportion of success is equally likely.

Assume you just flipped a coin four times and the result was heads, tails, tails, heads. If you code heads as a success and tails
as a failure then the following R code runs prop_model with the data.

Looking at the final probability distribution at n=4, what information does the model have regarding the underlying proportion of head? 
- it's mostly likely around 50%, but there is large uncertainly

### Zombie drugs with prop_model
If we really were interested in the underlying proportion of heads of the coin then prop_model isn't partially useful. 
Since it assumes that any underlying proportion of success is equally likely prior to seeing any data it will take a lot of coin flipping to convince
prop_model that the coin is fair. This model is more appropriate in a situation where we have little background knowledge about the underlying 
proportion of success. 

Let's say the zombie apocalypse is upon us and we have come up with a new experiment drug to cure zombies. We have no clue how effective it's going to be, but when we gave it to 13 zombies two of them turned humans again. 

- Change the data argument to prop_model to estimate the underlying proportion of success of curing a zombie. 

The model implemented is pop_model makes more sense here as we had no clue how good the drug would be. The final probability distribution (at n=13)
represents what the model now knows about the underlying proportion of cured zombies. What proportion of zombies would we expect to turn human if we
administered this new drug to the whole zombie population
- between 5% to 40%

# Prior and Posterior
- A prior is a probability distribution that represents what the model knows before seeing the data
- A posterior is a probability distribution that represents what the model knows after having seen the data

### Looking at samples from prop_model
Here again is the prop_model function which has been given the data from our zombie experiment where two out of 13 zombies got cured. In additional
to producing a plot, prop_model also returns a large random sample from the posterior over the underlying proportion of success. 

Assign the return value of prop_model to a variable called posterior and take a look at the first number of samples using the command head(posterior)

Looking at these first few samples confirms what is already show in the plot. That the underlying proportion of cured Zombies is likely somewhere between 
5% and 50%. But these were just the first six samples in posterior which currently contain 10,000 samples (the default of prop_model). 
- Take a look at the distribution of all the samples in posterior by plotting it as a histogram using the hist() function with posterior as the fist argument

Compare this histogram to the plot produced directly by prop_model (you'll find it if you go to the previous plot in the Plots tab). You should notice that 
histogram and the posterior distribution (at n = 13) describes the same distribution. 

OK, so you got a vector of samples from prop_model and confirmed that they are so good representation of the posterior distribution. What have you gained by doing this? So far, not much, but the next exercise will show you some examples of why samples are so convenient to work with. But before tou leave:

### Summarizing the zombie drug experiments 
The point of working this samples from a probability distribution is that it makes it easy to calculate new measures if interest. The following tasks are about doing just this! 

A point estimate is a single number used to summarize what's known about a parameter of interest. It can be seen as a "best guess" of the value of the parameter. A commonly used point estimate is the median of the posterior. It's the midpoint of the distribution, and it's equally probably for the distribution, and it's equally probably for the parameter value to be larger than the median as it is to be smaller that it. 

So, a best guess is that the drug would care around 18% of all zombies. Another common summary is to report an interval that includes the parameter
of interest with a certain probability. This is called a credible interval (CI). With a posterior represented as a vector of samples you can calculate a CI using the Quantile() function.

quantile() takes the vector of samples as its first argument and the second argument is a vector defining how much probability should be left below and above the CI. For example, the vector c(0.05, 0.95) would yield a 90% CI and c(0.25, 0.75)would yield a 50% CI. 

According to the credible interval, there is a 90% probability that the proportion of zombies the drug would cure is between 6% and 38%. (Here we have to be careful to remember that the percentage of cured zombies and the percentage of probability are two different things.)

Now, there is a rival zombies lab that is also working on a drug. They claim that they are certain that their drug cures 7% of the zombies it's administered to. Can we calculate how probable it is that our drug is better? Yes, we can! But it's a two stage process. 

- First, use the sum to count how many samples in posterior that are larger than 7%. Do this by giving posterior > 0.07 as the argument to sum.
- To turn this count into a probability we now need to normalize it, that is, divide it by the total number of samples in posterior. 
- Divide the result of sum by the number of samples in posterior
- You can get the number of samples in posterior using the length function

The result in a journal
Given the data of two cured and 11 relapsed zombies, and using the Bayesian model described before, there is a 90% probability that our drug cures between 6% and 39% of treated zombies. Further, there  is 93% probability that our drug cures zombies at a higher rate than the current state of the art drug. 


Bayesian Inference
- Data
- Generative model 
- Priors

What is a generative model? 
Its any kind of computer program, mathematical expression, or set of rules that you can feed fixed paramete values and that you can use to generate simulated data- Uniform distribution

```{r Prop_success}
# Parameters
prop_success <- 0.15
n_zombies <- 13

# Simulating data
data <- c()
for(zombies in 1:n_zombies){
  data[zombies] <- runif(1, min=0, max = 1) < prop_success
}

data <- as.numeric(data)
print(data)
```
## Generative model
- Assuming the underlying proportion of success of curing a zombie is 42% and that you administer th drug yo 100 zombies
- Instead of representing cured zombies as a vector of 1s and 0s it could be represented as a count of the number of cured out of the total number of treated. 
- Run this code to generate a new simulated dataset, but first change the code to count how many zombie in data were cured. Use the sum function for this and assign this count to data instead of vector of 1s and 0s.
```{r Generative model}
# The generative zombie drug model
# Set parameter 
prop_success <- 0.42
n_zombies <- 100 

# Simulating data
data <- c()
for (zombie in 1: n_zombies) {
  data[zombie] <- runif(1, min =0, max =1) < prop_success
}

data <- as.numeric(data)
data <- sum(data)
print(data)
```
Take the binomial distribution
Its turns out that generative model you ran last exercise already has a name. It's called the binomial process or the binomial distribution. In R you can use the rbinom function to simulate data from a binomial distribution. The rbinom function takes three arguments: 
- n The number of time you want to run the generative model
- size The number of trails. (For example, the number of zombies you've giving the drug)
- prob The underlying proportion of success as a number between 0 and 1
- Replicate the result from the last exercise using the rbinom function: Simulate one count the number of cured zombies out of 100 treated. Where the underlying proportion of success is 42%. 
Now, change the code to run the simulatation 200 times, rather that just one time. 

```{r binomial distribution}
rbinom(n=200, size = 100, prob=0.42)

```
How many visitors could you site get? 
To get more visitors to your website you are considering paying for an add to be shown 100 times on a popular social media site. According to the social media site, their ads get clicked on 10% of the time. 
Assume that 10% is a reasonable number, and assume that the binomial distribution is a reasonable generative model for how people click on ads. 
- Fill in the missing parameters and use the rbinom function to generate a sample that represents the probability distribution over what the number of visitors to your site is going to be. 
- Visualize this distribution using hist
```{r visitors example1}
# Fill in the parameters
n_samples <- 100000
n_ads_shown <- 100 
proportion_clicks <- 0.1

n_visitors <- rbinom(n_samples, size=n_ads_shown, prob=proportion_clicks)

# Visualize n_visitors
hist(n_visitors)

```
Based on the histogram we can expect the ad camping to result in at least 5 visitors to your site, with the probability of 90%

### Represneting uncertaintly with priors

```{r Priors}
n_samples <- 6
n_ads_shown <- 100 
proportion_clicks <- runif(n= 6, min=0.0, max = 1.0)
n_visitors <- rbinom(n_samples, size = n_ads_shown, prob = proportion_clicks)

n_visitors

```
### Adding a prior to the model
You're not sure that your ad will get clicked on exactly 10% of the time. Instead of assigning proportion_clicks a single value you are now going to assign it a large number of values drawn from a probability distribution.
For now, we are going to assume that it's equally likely that proportion_clicks could be as low as 0% or as high as 20%. These assumptions translate into  a uniform distribution which you can sample from in R like this:
- Because the rbinom function is vectorized the first value of proportion_clicks is used to sample the first value in n_visitors, the second value in proportion_clicks is used for the second in n_visitors, and so on. The result is that the samples in n_visitors now also incorporate the uncertaintly in what the underlying proportion of clicks could be. 
- First, visualize the uncertainly is proportion_clicks using the hist function
```{R prior}
x <- n_samples <- 10000
n_ads_shown <- 100

proportion_clicks <- runif(n= n_samples, min=0, max = 0.2)
n_visitors <- rbinom(n=n_samples, size = n_ads_shown, prob = proportion_clicks)
hist(proportion_clicks)
hist(n_visitors)
```
### Update a Bayesian model with data
you ran your ad camping, and 13 people clicked and visited your site when the ad was shown a 100 times. You would now like to use this new information to update the Bayesian model. 
- The model you put together in the last exercise resulted in two vectors: (1) proportion_clicks that represented the uncertainly regarding the underlying proportion of clicks and (2) n_visitors which represents the uncertainly regarding the number of visitors you would get. We have now put these vectors into a data frame for you called prior.

```{r Update a Bayesian model with data}

# Create the prior data frame
prior <- data.frame(proportion_clicks, n_visitors)

#Examine the prior data frame
head(prior)

# Create the posterior data frame
posterior <- prior[prior$n_visitors == 13, ]

# Visualize posterior proportion clicks
hist(posterior$proportion_clicks)

```

The reason we've called it prior is because it represents the uncertainty before (that is, prior to) having included the information in the data. Let's do that now!
prior$visitors represents the uncertainty over how many visitors you would get because of the ad campaign. But now you know you got exactly 13 visitors. 
- Update prior to include this information by conditioning on this data. That is, filtering away all rows where prior$visitor isn't equal to 13. Store the resulting data frame in posterior. 

Remember that you can subset data frame using [] -operator in R. For example, the following would return only those rows where n_visitors == 13:

This does;t look at all like the uniform distribution between 0.0 and 0.2 we put into proportion_clicks before. The whole distribution over what proportion_click could be. 

Looking at the probability distribution over proportion_clicks what does the model know about the underlying proportion of visitor clicking on the ad?
Its likely between 7% and 19%

### How many visitors could your site get(3)?
In the last exercise, you updated the probability distribution over the underlying proportions of clicks (proportion_clicks) using the new data. Now we want to use this updated proportion_clicks to predict how many visitors we would get if we reran the ad camping. 

The result from the last exercise is still in the data frame posterior, but if you at posterior$n_visitor you'll see it's just 13  repeated over and over again. This makes sense as posterior represents what the model knew about the outcome of the last ad campaign after having seen the data.

- Assign posterior to a new variable called prior which will represent the uncertainly regarding the new ad campaign you haven't run yet. 
- Take a look at the first rows in prior using the head() function
- While proportion_clicks represents the uncertainly regarding the underlying proportion of clicks for the next ad campaign. n_visitors has not been updated yet. 
  - Replace prior$n_visitors with a new sample drawn using rbinom with prior$proportion_clicks as an argument
  - plot the resulting prior$n_visitors using hist
  
According to the new model the probability of getting five or more visitor is 99%


```{r}
# Assign posterior to a new variable called prior
prior <- posterior

# Take a look at the first rows in prior
head(prior)

# Replace prior$n_visitors with a new sample and visualize the result
n_samples <-  nrow(prior)
n_ads_shown <- 100
prior$n_visitors <- rbinom(n_samples, size = n_ads_shown,
                           prob = prior$proportion_clicks)
hist(prior$n_visitors)

# Calculate the probability that you will get 5 or more visitors
sum(prior$n_visitors >= 5)/length(prior$n_visitors)


```

### Explore using the Beta distribution as a prior
The Beta distribution is useful probability distribution when you want model uncertainly over a parameter bounded between 0 and 1. Here you'll explore how the two parameters of the beta distribution its shape. 

One way to see how the shape parameters of the Beta distribution affect its shape is to generate a large number of random draws using the beta(n, shape1, shape2) function and visualize these as a histogram. The code to the right generates, 1,000,000 draws from a beta(1,1) distribution

### Pick the prior that best capatures the information
The new information you got from the social media company was:
Most ads get clicked on 5% of the time, but for some ads it is as low as 2% and for other as high as 8% 
There are many different probability distributions that one can argue captures this information. 
- Out of the four Beta distribution shown below, which captures this information the best?

```{r Beta distribution}
hist(rbeta(n = 1000000, shape1=3, shape2=12))
hist(rbeta(n = 1000000, shape1=15, shape2=75))
hist(rbeta(n = 1000000, shape1=5, shape2=95)) 
hist(rbeta(n = 1000000, shape1=30, shape2=10))


```
### Why use Bayesian data analysis
- You can include information source in addition to the data
- You can make any comparisons between groups or datasets
- You can use the result of a Bayesian analysis to do Decision Analysis
- You can change the underlying statistical model

### Change the model to use as informative prior
The code to the right is the old model you developed from scratch in chapter 2
- Change this model to use the new informative prior for proportion_clicks that you just selected. 

```{r Informative prior}
# Change the prior on proportion_click
n_draws = 1000000


proportion_clcks <- rbeta(n_draws, shape1=5, shape2=95)

n_visitors <- rbinom(n_draws, size= n_ads_shown, prob = proportion_clcks)

prior <- data.frame(proportion_clcks, n_visitors)

posterior <- prior[prior$n_visitors == 13, ]


# plot prior and the posterior distribution
par(mfcol = c(2,1))
hist(prior$proportion_clcks, xlim = c(0, 0.25))
hist(posterior$proportion_clcks, xlim = c(0, 0.25))

```
Due to the new informative prior it has shifted to the left, favoring lower rates. 

### Fit the model using another dataset
Let's fit the binomial model to both the video ad data (13 out of 100 clicked) and the new text ad data (6 out of a 100 clickes)

```{r Fit model using another dataset}

# Define parameters 
n_draws <- 100000 
n_ads_shown <- 100 
proportion_clcks <- runif(n_draws, min =  0.0, max = 0.2)
n_visitors <- rbinom(n = n_draws, size = n_ads_shown, prob= proportion_clicks)

prior <- data.frame(proportion_clicks, n_visitors)

# Create the posteriors for video and text ads
posterior_video <- prior[prior$n_visitors == 13, ]
posterior_text <- prior[prior$n_visitors == 6,]

# Visualize the posteriors
hist(posterior_video$proportion_clicks, xlim = c(0, 0.25))
hist(posterior_text$proportion_clicks, xlim = c(0, 0.25))

# Calculate the posterior difference: video_prop - text_prop

posterior <- data.frame(
  video_prop = posterior_video$proportion_clicks[1:4000],
  text_prop = posterior_text$proportion_clicks[1:4000],
  prop_diff = posterior_video$proportion_clicks[1:4000] - posterior_text$proportion_clicks[1:4000]
)

# Visualize prop_diff

hist(posterior$prop_diff)

# Calculate the mean of prop_diff

median(posterior$prop_diff)

# Calculate the proportion
sum(posterior$prop_diff > 0) /length(posterior$prop_diff)


```
### Decision Analysis

```{r Probability Distribution}

video_cost <- 0.25
text_cost <- 0.05
visitor_spend <- 2.53

posterior$video_profit <- posterior$video_prop * visitor_spend - video_cost
posterior$text_profit <- posterior$text_prop * visitor_spend - text_cost
posterior$profit_diff <- posterior$video_profit - posterior$text_profit


posterior

# Visualize the video_profit, text_profit  and profit_diffcolumns
hist(posterior$video_profit)
hist(posterior$text_profit)
hist(posterior$profit_diff)

# Calculate a "best guess" for the difference in profits
median(posterior$profit_diff)


# Calculate the probability that text ads are better than video ads
sum(posterior$profit_diff < 0)/length(posterior$profit_diff) 


```
Even though text ads get a lower proportion of clicks, they are also much cheaper. And, as you have calculated, there is a 63% probability that text ads are better. 

### Completely switch out the bionomal model
- Why? Well, you have some new data
- A banner ad for your site
- You don't pay per view, you pay per day
- A trial resulted in 19 clicks in a day
- How many daily site visits, should we expect, on average, if we pay for this banner?

Now it's obvious that the binomial model won't directly work here, it models the number of successes out of total number of shown ads, but now the ad is shown all the time, and we just have the daily count. 

A model for counts per day
- split the day into 1440 minutes
- What proportion od minutes results in a click on the ad? 
- Split the day into 86000 seconds
- What proportion of seconds results in a click on the ad?
- Split the day into 8640000 milliseconds
- What proportion of milliseconds results in a click on the ad?
- split the day into infinite parts

This process is called poisson distribution
- One parameter: The mean number of events per time unit.
- rpois samples from the Possion distribution

```{r}
n_clicks <- rpois(n=100000, lambda = 20)

hist(n_clicks)

```















